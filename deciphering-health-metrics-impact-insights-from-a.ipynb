{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"deepnote_persisted_session":{"createdAt":"2024-01-02T19:30:17.908Z"},"deepnote_full_width":true,"deepnote_notebook_id":"d7d687e549f14126808f4f100e5cc1ba","deepnote_execution_queue":[],"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7335195,"sourceType":"datasetVersion","datasetId":4258289}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Deciphering Health Metrics Impact: Insights from a Random Forest Model**\n\n\nBackground:\nThe utilized dataset file is from a study conducted by Professor Vera Novak and colleagues [(Novak et al., 2022)](https://physionet.org/content/cerebral-perfusion-diabetes/1.0.1/). This study investigates the effects of inflammation on perfusion regulation and brain volumes in type 2 diabetes. \n\nObjective: \nThis data analysis project is focused on exploring the complex relationships between diabetes, inflammation, brain health, and physical function. The analysis and machine learning modeling using this dataset can provide insights into the pathological mechanisms of diabetes-related brain changes and their possible clinical manifestations. Developing and interpreting a Machine Learning model for a healthcare-related dataset. One of the critical goals is to understand the influence of specific health metrics  on the model's predictions. This analysis aims to uncover trends, patterns, or non-linear relationships between these features and the model's decision-making process.\n\n\nData Description (Summary)\n- Participant Demographics: Data on age, diabetes status (diabetic or nondiabetic), and other demographic details.\n- Clinical Measurements: Measurements of serum soluble vascular and intercellular adhesion molecules, which serve as markers of endothelial integrity, results that indicate vascular integrity and functioning.\n- Cognitive and Physical Assessments: Data on cognitive functions, depression levels, and physical capabilities like walking speed.\n\n\n\nExpected Outcome: \nAdoption and utilization of statistical methods and Machine Learning models, we aim to identify any significant trends or thresholds in the data that could inform medical professionals or data scientists about the critical factors influencing health outcomes as predicted by the analysis and model. This might be useful in providing leads to better-informed healthcare decisions and potentially improved patient care strategies.","metadata":{"formattedRanges":[],"cell_id":"7ca6da6f07c74bf5835b2019ae902e4b","deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"**Exploratory Data Analysis (EDA)**\n\n\nSteps executed:\n\n1. Data Summarization: Understanding the basic structure of the dataset, including the number of rows and columns, types of variables, and missing values. \n\n2. Univariate Analysis: Analyzing the distribution of individual variables.\n\n3. Bivariate or Multivariate Analysis: Exploring relationships between variables.\n\n4. Identification of Trends, Patterns, and Outliers: Looking for any unusual data points or important patterns in the data.","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Loading the data\nUPLOADED_FILE = '/kaggle/input/novak-datafile/Novak et.Jal_DATA-FILE_CSV.csv'\ndata = pd.read_csv(UPLOADED_FILE)\n\n# Displaying the first few rows of the data\ndata.head()","metadata":{"source_hash":null,"execution_start":1704222209056,"execution_millis":2468,"deepnote_to_be_reexecuted":false,"cell_id":"951ff69fc7ac4828970896e898f80c38","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2024-02-15T14:30:02.447222Z","iopub.execute_input":"2024-02-15T14:30:02.447679Z","iopub.status.idle":"2024-02-15T14:30:02.493377Z","shell.execute_reply.started":"2024-02-15T14:30:02.447615Z","shell.execute_reply":"2024-02-15T14:30:02.492129Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Summarization: Basic structure, types of variables, and missing values\n\n# Basic structure\ndata_shape = data.shape\n\n# Types of variables\ndata_types = data.dtypes\n\n# Missing values\nmissing_values = data.isnull().sum()\n\n# Preparing a summary for display\nsummary = {\n    \"Total Rows\": data_shape[0],\n    \"Total Columns\": data_shape[1],\n    \"Column Names\": data.columns.tolist(),\n    \"Data Types\": data_types.value_counts().to_dict(),\n    \"Columns with Most Missing Values\": missing_values.idxmax(),\n    \"Number of Missing Values in Most Incomplete Column\": missing_values.max(),\n    \"Total Missing Values\": missing_values.sum()\n}\n\nsummary\n\n","metadata":{"source_hash":null,"execution_start":1704222211526,"execution_millis":36,"deepnote_to_be_reexecuted":false,"cell_id":"ddaf2d4014c24bc99ce59975300be5bb","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2024-02-15T14:30:05.497029Z","iopub.execute_input":"2024-02-15T14:30:05.497439Z","iopub.status.idle":"2024-02-15T14:30:05.513000Z","shell.execute_reply.started":"2024-02-15T14:30:05.497406Z","shell.execute_reply":"2024-02-15T14:30:05.511862Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**The initial exploratory data analysis (EDA) provides the following insights into the dataset:**\n\n*     Total Rows: There are 121 rows in the dataset.\n\n*     Total Columns: The dataset contains 170 columns.\n\n*     Data Types: The majority of the columns (112) are of float type, followed by 47 object (string) type columns, and 11 integer type columns.\n\n*     Columns with Most Missing Values: The column 'STROKE YR PATIENT MEDICAL HISTORY' has the most missing values, with all 121 entries missing.\n\n*     Total Missing Values: There are 5665 missing values in the entire dataset.\n\nGiven the high number of columns, a wide range of variables is covered, including patient medical history, laboratory test results, and specific measurements related to diabetes and associated complications.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Filter out some key columns for univariate analysis\n# Here we are selecting a few columns that might be of interest based on the study focus\nkey_columns = [\n    'GLUCOSE mg/dL', 'HDL mg/dL', 'LDL CALCmg/dL', 'TRIGLYCmg/dL',\n    '24Hour-Daytime-SBP', '24Hour-Nighttime-SBP', '24Hour-Daytime-DBP', \n    '24Hour-Nighttime-DBP', 'Gait (Normal) RPE Start (1-10)', 'Gait (Normal) RPE End (1-10)'\n]\n\n# Selecting only the key columns for univariate analysis\nselected_data = data[key_columns]\n\n# Univariate Analysis: Distribution plots for the selected variables\nplt.figure(figsize=(15, 10))\nfor i, column in enumerate(selected_data.columns, 1):\n    plt.subplot(3, 4, i)\n    sns.histplot(selected_data[column].dropna(), kde=True)\n    plt.title(column)\n    plt.tight_layout()\n\nplt.show()\n","metadata":{"source_hash":null,"execution_start":1704222211526,"execution_millis":3955,"deepnote_to_be_reexecuted":false,"cell_id":"c6b5e68957de47cfba0afa8917c7ca4b","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2024-02-15T14:30:35.234507Z","iopub.execute_input":"2024-02-15T14:30:35.234949Z","iopub.status.idle":"2024-02-15T14:30:39.531834Z","shell.execute_reply.started":"2024-02-15T14:30:35.234915Z","shell.execute_reply":"2024-02-15T14:30:39.530451Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Univariate Analysis of Key Health Metrics**\n\nOverview:\nThis section of our analysis is dedicated to examining the distribution of selected health-related variables individually. The aim is to gain a deeper understanding of each variable's characteristics and distribution in the dataset.\n \n \n1. Glucose Levels (mg/dL):\n    * Significance: Glucose levels are a primary indicator of metabolic health, crucial in diagnosing and managing diabetes. Normal, pre-diabetic, \n      and diabetic conditions can be inferred from these levels.\n    * Expected Insights: We aim to observe the distribution of glucose levels, looking for signs of normalcy or skewness towards higher levels, \n      which might indicate a prevalence of glucose intolerance or diabetes in the population.\n      \n\n2. HDL Cholesterol (mg/dL):\n    * Significance: High-density lipoprotein (HDL) cholesterol is known as \"good\" cholesterol. Higher levels are often associated with a lower risk \n      of heart disease.\n    * Expected Insights: The analysis will focus on the distribution of HDL levels, where higher values are generally desirable. A \n      left-skewed distribution could suggest a healthier profile in the population.\n      \n\n3. LDL Cholesterol (mg/dL):\n    * Significance: Low-density lipoprotein (LDL) is often termed as \"bad\" cholesterol. High levels of LDL are linked to an increased risk of \n      heart disease.\n    * Expected Insights: The distribution of LDL levels will be examined, with particular attention to any right-skewed patterns indicating prevalent \n      high LDL levels, a risk factor for cardiovascular diseases.\n      \n\n4. Triglycerides (mg/dL):\n    * Significance: Triglycerides are a type of fat found in the blood. High levels are associated with an increased risk of heart disease, \n      especially when coupled with low HDL and high LDL levels.\n    * Expected Insights: The goal is to assess the spread and central tendency of triglyceride levels, identifying any trends towards higher values \n      that might indicate metabolic health risks.\n      \n\n5. Blood Pressure Readings:\n    * Significance: Blood pressure is a vital sign indicating the force of blood against artery walls. Both high (hypertension) and \n      low (hypotension) readings have significant health implications.\n    * Variants: '24Hour-Daytime-SBP', '24Hour-Nighttime-SBP', '24Hour-Daytime-DBP', '24Hour-Nighttime-DBP'.\n    * Expected Insights: We'll explore the distribution of systolic (SBP) and diastolic (DBP) blood pressures during different \n      times (daytime and nighttime). Patterns of elevated or reduced blood pressure can be indicative of underlying health conditions.\n      \n      \n \n6. Gait (Normal) RPE Start/End (1-10):\n     * Significance: Gait analysis, especially the Rating of Perceived Exertion (RPE), provides insights into an individual's functional mobility \n       and fitness. It can be crucial for assessing the physical condition, especially in older adults or those with mobility issues.\n     * Expected Insights: The analysis will reveal patterns in perceived exertion levels, which can highlight common physical fitness levels or \n       mobility challenges within the study group.\n\n\n\n\n\nThis detailed examination of each variable will shed light on various aspects of the study population's health. Understanding the distribution and tendencies of these crucial health metrics will provide a foundational understanding of the population's overall health status and potential risk factors.\n","metadata":{}},{"cell_type":"code","source":"# Bivariate/Multivariate Analysis: Correlation Matrix\n# Adjusting the selection of columns for bivariate analysis (excluding 'HbA1c %')\nadjusted_columns = [\n    'Right Eye Diabetic Retinopathy', 'Left Eye Diabetic Retinopathy', \n    'Macular Edema (more advanced eye)'\n]\n\n# Combining the selected columns for the analysis\nadjusted_bivariate_columns = key_columns + adjusted_columns\nadjusted_bivariate_data = data[adjusted_bivariate_columns]\n\n# Converting categorical variables to numeric for correlation analysis\n# Diabetic retinopathy and macular edema columns are converted\nadjusted_bivariate_data['Right Eye Diabetic Retinopathy'] = adjusted_bivariate_data['Right Eye Diabetic Retinopathy'].apply(lambda x: 1 if x == 'Yes' else 0)\nadjusted_bivariate_data['Left Eye Diabetic Retinopathy'] = adjusted_bivariate_data['Left Eye Diabetic Retinopathy'].apply(lambda x: 1 if x == 'Yes' else 0)\nadjusted_bivariate_data['Macular Edema (more advanced eye)'] = adjusted_bivariate_data['Macular Edema (more advanced eye)'].apply(lambda x: 1 if x == 'Yes' else 0)\n\n# Computing the correlation matrix for the adjusted dataset\nadjusted_correlation_matrix = adjusted_bivariate_data.corr()\n\n# Plotting the correlation matrix\nplt.figure(figsize=(12, 8))\nsns.heatmap(adjusted_correlation_matrix, annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix of Adjusted Selected Variables')\nplt.show()\n","metadata":{"source_hash":null,"execution_start":1704222215482,"execution_millis":514,"deepnote_to_be_reexecuted":false,"cell_id":"69e6ab4880f0438fb0642c21ec59eaa3","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2024-02-15T14:30:56.780608Z","iopub.execute_input":"2024-02-15T14:30:56.781034Z","iopub.status.idle":"2024-02-15T14:30:57.572219Z","shell.execute_reply.started":"2024-02-15T14:30:56.781001Z","shell.execute_reply":"2024-02-15T14:30:57.570498Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Univariate Analysis of Key Health Metrics**\n\nOverview:\nThis section of our analysis is dedicated to examining the distribution of selected health-related variables individually. The aim is to gain a deeper understanding of each variable's characteristics and distribution in the dataset.\n \n \n1. Glucose Levels (mg/dL):\n    * Significance: Glucose levels are a primary indicator of metabolic health, crucial in diagnosing and managing diabetes. Normal, pre-diabetic, \n      and diabetic conditions can be inferred from these levels.\n    * Expected Insights: We aim to observe the distribution of glucose levels, looking for signs of normalcy or skewness towards higher levels, \n      which might indicate a prevalence of glucose intolerance or diabetes in the population.\n      \n\n2. HDL Cholesterol (mg/dL):\n    * Significance: High-density lipoprotein (HDL) cholesterol is known as \"good\" cholesterol. Higher levels are often associated with a lower risk \n      of heart disease.\n    * Expected Insights: The analysis will focus on the distribution of HDL levels, where higher values are generally desirable. A \n      left-skewed distribution could suggest a healthier profile in the population.\n      \n\n3. LDL Cholesterol (mg/dL):\n    * Significance: Low-density lipoprotein (LDL) is often termed as \"bad\" cholesterol. High levels of LDL are linked to an increased risk of \n      heart disease.\n    * Expected Insights: The distribution of LDL levels will be examined, with particular attention to any right-skewed patterns indicating prevalent \n      high LDL levels, a risk factor for cardiovascular diseases.\n      \n\n4. Triglycerides (mg/dL):\n    * Significance: Triglycerides are a type of fat found in the blood. High levels are associated with an increased risk of heart disease, \n      especially when coupled with low HDL and high LDL levels.\n    * Expected Insights: The goal is to assess the spread and central tendency of triglyceride levels, identifying any trends towards higher values \n      that might indicate metabolic health risks.\n      \n\n5. Blood Pressure Readings:\n    * Significance: Blood pressure is a vital sign indicating the force of blood against artery walls. Both high (hypertension) and \n      low (hypotension) readings have significant health implications.\n    * Variants: '24Hour-Daytime-SBP', '24Hour-Nighttime-SBP', '24Hour-Daytime-DBP', '24Hour-Nighttime-DBP'.\n    * Expected Insights: We'll explore the distribution of systolic (SBP) and diastolic (DBP) blood pressures during different \n      times (daytime and nighttime). Patterns of elevated or reduced blood pressure can be indicative of underlying health conditions.\n      \n      \n \n6. Gait (Normal) RPE Start/End (1-10):\n     * Significance: Gait analysis, especially the Rating of Perceived Exertion (RPE), provides insights into an individual's functional mobility \n       and fitness. It can be crucial for assessing the physical condition, especially in older adults or those with mobility issues.\n     * Expected Insights: The analysis will reveal patterns in perceived exertion levels, which can highlight common physical fitness levels or \n       mobility challenges within the study group.\n\n\n\n\n\nThis detailed examination of each variable will shed light on various aspects of the study population's health. Understanding the distribution and tendencies of these crucial health metrics will provide a foundational understanding of the population's overall health status and potential risk factors.\n","metadata":{}},{"cell_type":"code","source":"# Outlier Detection using Boxplots for the key variables\n\nplt.figure(figsize=(15, 10))\n\n# Creating boxplots for each of the key variables\nfor i, column in enumerate(selected_data.columns, 1):\n    plt.subplot(3, 4, i)\n    sns.boxplot(y=selected_data[column])\n    plt.title(column)\n    plt.tight_layout()\n\nplt.show()\n\n# Function for detecting outliers using the Interquartile Range (IQR) method\ndef detect_outliers(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n    return outliers\n\n# Detecting outliers for each of the key variables\noutliers_dict = {}\nfor column in selected_data.columns:\n    outliers = detect_outliers(selected_data, column)\n    outliers_dict[column] = outliers\n\n# Displaying the number of outliers for each variable\noutliers_count = {column: len(outliers_dict[column]) for column in outliers_dict}\noutliers_count\n\n","metadata":{"source_hash":null,"execution_start":1704222215997,"execution_millis":1525,"deepnote_to_be_reexecuted":false,"cell_id":"217f4cc9dca54137aff1cecbb592a065","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2024-02-15T14:31:26.457730Z","iopub.execute_input":"2024-02-15T14:31:26.458208Z","iopub.status.idle":"2024-02-15T14:31:29.519831Z","shell.execute_reply.started":"2024-02-15T14:31:26.458170Z","shell.execute_reply":"2024-02-15T14:31:29.518697Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Outlier Detection in Key Health Variables**\n\nPurpose:\nIn this analysis, we focus on identifying outliers in crucial health metrics. Outliers are data points that differ significantly from other observations and could indicate measurement errors, unique conditions, or data entry anomalies.\n\n1. Methodology:\n   * Technique: Boxplots were used for outlier detection. Boxplots are a standard statistical tool for visualizing the distribution of data \n     and identifying points that fall outside the typical range (i.e., outliers).\n   * Variables Analyzed: Key health metrics including glucose levels, lipid profiles (HDL, LDL, Triglycerides), blood pressure readings \n     (daytime and nighttime), and gait-related ratings (RPE Start/End).\n \n\n2. Outlier Count by Variable: \n    * Glucose (mg/dL): 3 outliers were detected, suggesting unusual glucose levels.\n    * HDL (mg/dL): 3 outliers, indicating atypical HDL cholesterol readings.\n    * LDL (mg/dL): No outliers were found, implying consistent LDL cholesterol levels.\n    * Triglycerides (mg/dL): 4 outliers, highlighting some extreme triglyceride readings.\n    * 24-Hour Daytime SBP: 7 outliers, indicating notable deviations in daytime systolic blood pressure.\n    * 24-Hour Nighttime SBP: 1 outlier, less variation in nighttime systolic blood pressure.\n    * 24-Hour Daytime DBP: 4 outliers in daytime diastolic blood pressure.\n    * 24-Hour Nighttime DBP: 3 outliers in nighttime diastolic blood pressure.\n    * Gait RPE Start (1-10): 17 outliers, a significant count, suggesting varied physical exertion levels at the start of the gait test.\n    * Gait RPE End (1-10): 4 outliers, indicating variations in exertion by the end of the gait test.\n\n\n3. Interpretation and Action Points:\n    * Potential Causes: Outliers may stem from a range of factors, including measurement errors, data entry issues, or true anomalies in the health \n      status of individuals.\n    * Impact on Analysis: Outliers can skew analysis results, especially in models sensitive to extreme values. It's crucial to investigate them to \n      ensure the robustness of subsequent analyses.\n    * Next Steps: Determine whether to retain, modify, or remove these outliers based on their nature and the goals of the study. In medical  \n      datasets, outliers can sometimes represent clinically significant cases worth exploring separately.\n\n\n\nDetecting outliers is a vital step in data preprocessing, particularly for healthcare data. Understanding the nature of these outliers will aid in refining our analysis and ensuring the accuracy of our findings.","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries for encoding\nfrom sklearn import preprocessing\n\n# Create a label encoder object\nle = preprocessing.LabelEncoder()\n\n# First, we need to define X. In this case, let's assume X is the entire dataframe without the 'GLUCOSE mg/dL' column.\nX = data.drop('GLUCOSE mg/dL', axis=1)\n\n# Since our matrix of features X contains categorical values, we should encode them into numbers\nX = X.apply(le.fit_transform)\n\n# Since we are now focusing on a binary classification task (high risk vs. low risk),\n# we need to define a binary target variable. \n\n# Creating a synthetic binary target variable\n# Here, we will categorize patients as 'high risk' or 'low risk' based on a threshold.\nthreshold = data['GLUCOSE mg/dL'].median()\nbinary_target = data['GLUCOSE mg/dL'].apply(lambda x: 1 if x >= threshold else 0)\n\n# Splitting the dataset into the Training set and Test set with the new binary target\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train_binary, y_test_binary = train_test_split(X, binary_target, test_size=0.2, random_state=42)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n#Training the Random Forest Classifier with the binary target\nfrom sklearn.ensemble import RandomForestClassifier\nbinary_model = RandomForestClassifier()\nbinary_model.fit(X_train_scaled, y_train_binary)\n\n# Predicting the Test set results\ny_pred_binary = binary_model.predict(X_test_scaled)\n\n# Generating classification report and confusion matrix for the binary classification\nfrom sklearn.metrics import classification_report, confusion_matrix\nclassification_rep_binary = classification_report(y_test_binary, y_pred_binary)\nconfusion_mat_binary = confusion_matrix(y_test_binary, y_pred_binary)\n\n# ROC AUC Score for binary classification\nfrom sklearn.metrics import roc_auc_score\nroc_auc_binary = roc_auc_score(y_test_binary, binary_model.predict_proba(X_test_scaled)[:, 1])\n\nclassification_rep_binary, confusion_mat_binary, roc_auc_binary","metadata":{"source_hash":null,"execution_start":1704222217531,"execution_millis":494,"deepnote_to_be_reexecuted":false,"cell_id":"8912d418d0264d14a8d0e3b4dba87ed7","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2024-02-15T14:32:14.414507Z","iopub.execute_input":"2024-02-15T14:32:14.414952Z","iopub.status.idle":"2024-02-15T14:32:15.135311Z","shell.execute_reply.started":"2024-02-15T14:32:14.414919Z","shell.execute_reply":"2024-02-15T14:32:15.134108Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Outlier Detection in Key Health Variables**\n\nPurpose:\nIn this analysis, we focus on identifying outliers in crucial health metrics. Outliers are data points that differ significantly from other observations and could indicate measurement errors, unique conditions, or data entry anomalies.\n\n1. Methodology:\n   * Technique: Boxplots were used for outlier detection. Boxplots are a standard statistical tool for visualizing the distribution of data \n     and identifying points that fall outside the typical range (i.e., outliers).\n   * Variables Analyzed: Key health metrics including glucose levels, lipid profiles (HDL, LDL, Triglycerides), blood pressure readings \n     (daytime and nighttime), and gait-related ratings (RPE Start/End).\n \n\n2. Outlier Count by Variable: \n    * Glucose (mg/dL): 3 outliers were detected, suggesting unusual glucose levels.\n    * HDL (mg/dL): 3 outliers, indicating atypical HDL cholesterol readings.\n    * LDL (mg/dL): No outliers were found, implying consistent LDL cholesterol levels.\n    * Triglycerides (mg/dL): 4 outliers, highlighting some extreme triglyceride readings.\n    * 24-Hour Daytime SBP: 7 outliers, indicating notable deviations in daytime systolic blood pressure.\n    * 24-Hour Nighttime SBP: 1 outlier, less variation in nighttime systolic blood pressure.\n    * 24-Hour Daytime DBP: 4 outliers in daytime diastolic blood pressure.\n    * 24-Hour Nighttime DBP: 3 outliers in nighttime diastolic blood pressure.\n    * Gait RPE Start (1-10): 17 outliers, a significant count, suggesting varied physical exertion levels at the start of the gait test.\n    * Gait RPE End (1-10): 4 outliers, indicating variations in exertion by the end of the gait test.\n\n\n3. Interpretation and Action Points:\n    * Potential Causes: Outliers may stem from a range of factors, including measurement errors, data entry issues, or true anomalies in the health \n      status of individuals.\n    * Impact on Analysis: Outliers can skew analysis results, especially in models sensitive to extreme values. It's crucial to investigate them to \n      ensure the robustness of subsequent analyses.\n    * Next Steps: Determine whether to retain, modify, or remove these outliers based on their nature and the goals of the study. In medical  \n      datasets, outliers can sometimes represent clinically significant cases worth exploring separately.\n\n\n\nDetecting outliers is a vital step in data preprocessing, particularly for healthcare data. Understanding the nature of these outliers will aid in refining our analysis and ensuring the accuracy of our findings.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n# Training the Random Forest Classifier\nrf_model = RandomForestClassifier(random_state=42)\nrf_model.fit(X_train_scaled, y_train_binary)\n\n# Predicting the Test set results\ny_pred_rf = rf_model.predict(X_test_scaled)\n\n# Generating classification report and confusion matrix for the Random Forest model\nclassification_rep_rf = classification_report(y_test_binary, y_pred_rf)\nconfusion_mat_rf = confusion_matrix(y_test_binary, y_pred_rf)\n\n# ROC AUC Score for the Random Forest model\nroc_auc_rf = roc_auc_score(y_test_binary, rf_model.predict_proba(X_test_scaled)[:, 1])\n\nclassification_rep_rf, confusion_mat_rf, roc_auc_rf\n\n","metadata":{"source_hash":null,"execution_start":1704222218028,"execution_millis":105,"deepnote_to_be_reexecuted":false,"cell_id":"d0513c9962f74bf88fc5513329a4a596","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2024-02-15T14:32:53.237547Z","iopub.execute_input":"2024-02-15T14:32:53.238086Z","iopub.status.idle":"2024-02-15T14:32:53.443316Z","shell.execute_reply.started":"2024-02-15T14:32:53.238049Z","shell.execute_reply":"2024-02-15T14:32:53.442104Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Random Forest Classifier Performance Analysis**\n\nClassification Report:\n\nOur Random Forest Classifier demonstrates strong performance across two classes, as evidenced by the following metrics:\n- Class 0 (Precision: 0.94, Recall: 1.00, F1-Score: 0.97): High precision and perfect recall for \n  Class 0 suggest the model is highly effective in identifying and classifying this group with minimal     false positives.\n- Class 1 (Precision: 1.00, Recall: 0.90, F1-Score: 0.95): The perfect precision for Class 1 \n  indicates no false positives, though the recall is slightly lower, suggesting a small number of \n  false negatives.\n- Overall Accuracy: The model achieves an accuracy of 96%, indicating excellent overall performance.\n- Macro and Weighted Averages: Both averages hover around 0.96-0.97, signifying consistent \n  performance across both classes.\n\n\n\nConfusion Matrix:\nThe confusion matrix further illuminates the model's performance:\n-     True Positives for Class 0: 15 (Correctly predicted Class 0)\n-     True Negatives for Class 1: 9 (Correctly predicted Class 1)\n-     False Positives for Class 1: 0 (No incorrect predictions for Class 1)\n-     False Negatives for Class 0: 1 (One instance of Class 0 incorrectly predicted as Class 1)\n\nThis matrix confirms the model's strength in correctly identifying both classes, with a particularly strong ability to avoid false positives for Class 1.\n\n\n\nROC AUC Score:\nScore: 0.9767: This near-perfect score indicates the model's exceptional ability to discriminate between the two classes. A score close to 1.0 suggests a high true positive rate and a low false positive rate.\n\n\n\nThe Random Forest Classifier exhibits robust performance in classifying the given dataset. Its high precision and recall across both classes, combined with an impressive ROC AUC score, highlight its efficacy. The minimal number of false negatives and no false positives for Class 1 are particularly noteworthy. These results suggest that the model is well-tuned and reliable for this specific dataset, making it a strong candidate for further use or analysis in this context.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Defining the parameter grid for Random Forest\nparam_grid = {\n    'n_estimators': [50, 100, 200], # Number of trees in the forest\n    'max_depth': [None, 10, 20, 30], # Maximum depth of the tree\n    'min_samples_split': [2, 5, 10], # Minimum number of samples required to split a node\n    'min_samples_leaf': [1, 2, 4] # Minimum number of samples required at a leaf node\n}\n\n# Creating the Grid Search with Cross-Validation\ngrid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n                           cv=5, n_jobs=-1, scoring='roc_auc', verbose=2)\n\n# Fitting the grid search to the data\ngrid_search.fit(X_train_scaled, y_train_binary)\n\n# Best parameters and best score\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nbest_params, best_score\n\n","metadata":{"source_hash":null,"execution_start":1704222218138,"execution_millis":67644,"deepnote_to_be_reexecuted":false,"cell_id":"b95a724eaf8e4034a2ed939c0a22ac3b","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2024-02-15T14:33:31.901511Z","iopub.execute_input":"2024-02-15T14:33:31.902007Z","iopub.status.idle":"2024-02-15T14:34:17.875312Z","shell.execute_reply.started":"2024-02-15T14:33:31.901970Z","shell.execute_reply":"2024-02-15T14:34:17.874201Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Optimal Parameters and Performance for Random Forest**\n\nParameter Grid Tuning Results:\n\nOur tuning of the Random Forest model parameters using a grid search approach has identified the following optimal parameters:\n\n1. Maximum Depth (max_depth): None\n   This suggests that the best performance is achieved when the trees are allowed to expand until all leaves are pure or until all leaves contain less than  the min_samples_split samples.\n\n\n2. Minimum Samples per Leaf (min_samples_leaf): 1\n   A value of 1 indicates that each leaf node is allowed to hold only one sample, favoring a more detailed, fine-grained split of the data.\n\n\n3. Minimum Samples for a Split (min_samples_split): 10\n   This means the minimum number of samples required to split an internal node is 10. It's a balance between underfitting and overfitting.\n\n\n4. Number of Trees (n_estimators): 200\n   The optimal model includes 200 trees in the forest, providing a good balance between computational efficiency and model performance.\n\n\n5. Model Score:\n   * Best Score Achieved: 0.9943\n     The model achieved a near-perfect score with these parameters, indicating an exceptionally high level of predictive accuracy.\n \n\n\n\nInterpretation:\n \nThe combination of these parameters leads to a highly accurate Random Forest model. The absence of a maximum depth limit and a low threshold for samples per leaf suggest the model is complex and very detailed in its decision boundaries. The relatively higher number of trees (200) contributes to the robustness of the model, reducing the variance and improving generalizability.\n\nA high score like 0.9943 implies that the model is very effective in capturing the patterns in the dataset, with a strong predictive performance. However, it is important to be cautious about overfitting, especially given the unrestricted tree depth and the high number of estimators. Cross-validation and external validation on unseen data should be used to ensure that the model generalizes well.\n\nThis markdown provides a comprehensive interpretation of the optimal parameters for your Random Forest model and its performance based on the output from the grid search. It's crucial to consider these results in the context of your dataset and the specific problem you are addressing.","metadata":{}},{"cell_type":"markdown","source":"**Cross-Validation with Default Parameters:**\n\nThis was performed to conduct cross-validation on the Random Forest model with its current parameters to get a more robust evaluation of its performance.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# Performing Cross-Validation with the Random Forest model using its default parameters\n# We'll use ROC AUC as the scoring metric\ncv_scores = cross_val_score(rf_model, X_train_scaled, y_train_binary, cv=5, scoring='roc_auc')\n\n# Calculating the mean and standard deviation of the cross-validation scores\ncv_mean = cv_scores.mean()\ncv_std = cv_scores.std()\n\ncv_mean, cv_std\n\n","metadata":{"source_hash":null,"execution_start":1704222285791,"execution_millis":566,"deepnote_to_be_reexecuted":false,"cell_id":"cbd9459e1d084c10a79e3514a1c3e4bc","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2024-02-15T14:35:37.756065Z","iopub.execute_input":"2024-02-15T14:35:37.756465Z","iopub.status.idle":"2024-02-15T14:35:38.613001Z","shell.execute_reply.started":"2024-02-15T14:35:37.756435Z","shell.execute_reply":"2024-02-15T14:35:38.611661Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Cross-Validation Results for Random Forest Model**\n\nCross-Validation with Default Parameters:\nIn this stage, we performed cross-validation on the Random Forest model using its default parameters. The key objective was to assess the model's performance in a more robustly by evaluating it on multiple subsets of the data.\n\n1. Scoring Metric - ROC AUC:\n   * ROC AUC (Receiver Operating Characteristic Area Under the Curve) was chosen as the scoring metric. This metric is particularly useful for \n     binary classification problems as it evaluates the model's ability to distinguish between the two classes.\n\n2. Cross-Validation Scores:\n   * Average ROC AUC Score: 0.9800\n     This score, close to 1, suggests that the model has an excellent capability to differentiate between the classes across different subsets of \n     the data.\n     \n   * Standard Deviation: 0.0280\n     The relatively low standard deviation indicates that the model's performance is consistent across different folds of the cross-validation.\n\n\nInterpretation:\nThe high average ROC AUC score from cross-validation indicates that the Random Forest model, even with its default parameters, is highly effective in this classification task. The consistency of the model's performance, as shown by the low standard deviation, further reinforces its reliability and robustness.\n\nThis result is encouraging as it demonstrates the model's strong predictive power and its ability to generalize across various subsets of data. However, it is essential to consider this performance in the context of the specific problem domain and the characteristics of the dataset. In addition, validating the model on a completely independent test set would further confirm its effectiveness and generalizability.\n","metadata":{}},{"cell_type":"markdown","source":"**Feature Importance Analysis:**\n\nThis is to examine which features are most influential in the Random Forest model. This can provide insights into the factors most strongly associated with other health complications of the disease like dementia, stroke, or mortality.\n    ","metadata":{}},{"cell_type":"code","source":"# Feature Importance Analysis for the Random Forest model\n\n# Extracting feature importances\nfeature_importances = rf_model.feature_importances_\n\n# Creating a DataFrame for feature importances\nfeatures = X.columns\nfeature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n\n# Sorting the features by their importance\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\nfeature_importance_df\n\n","metadata":{"source_hash":null,"execution_start":1704222286370,"execution_millis":29,"deepnote_table_state":{"sortBy":[],"filters":[],"pageSize":10,"pageIndex":0},"deepnote_table_loading":false,"deepnote_to_be_reexecuted":false,"cell_id":"7429d1b6770b4da1868c8839f6c18eb2","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2024-02-15T14:36:47.033103Z","iopub.execute_input":"2024-02-15T14:36:47.033511Z","iopub.status.idle":"2024-02-15T14:36:47.057943Z","shell.execute_reply.started":"2024-02-15T14:36:47.033476Z","shell.execute_reply":"2024-02-15T14:36:47.057098Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, auc\n\n# Assuming the RandomForest model: rf_model has already been trained.\n# rf_model = RandomForestClassifier(...)\n# rf_model.fit(X_train, y_train)\n\n# Extracting feature importances\nfeature_importances = rf_model.feature_importances_\n\n# Getting feature names\nfeatures = X.columns\n\n# Creating a DataFrame for feature importances\nfeature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n\n# Sorting the features by their importance\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n# Plotting Feature Importances\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importances in Random Forest Model')\nplt.gca().invert_yaxis() # To display the most important feature at the top\nplt.show()\n\n# Compute ROC curve and AUC for Random Forest model\nfpr, tpr, thresholds = roc_curve(y_test_binary, rf_model.predict_proba(X_test_scaled)[:, 1])\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(7, 7))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"source_hash":null,"execution_start":1704222286416,"execution_millis":2050,"deepnote_to_be_reexecuted":false,"cell_id":"832a1de4e5c548bc9bbbfc0e9dbd92bb","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2024-02-15T14:36:52.043848Z","iopub.execute_input":"2024-02-15T14:36:52.044231Z","iopub.status.idle":"2024-02-15T14:36:54.322673Z","shell.execute_reply.started":"2024-02-15T14:36:52.044200Z","shell.execute_reply":"2024-02-15T14:36:54.321540Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Feature Importance Analysis in Random Forest Model**\n\nOverview:\nIn this analysis, we have identified the importance of various features used by the Random Forest model. This helps in understanding which factors are most influential in predicting the outcome.\n\n1. Top Influential Features:\n\n   * Fasting Glucose (mg/dL) (Visit 2 for GE-75): With an importance score of 0.116351, this feature stands out as the most significant predictor. \n     It underscores the critical role of fasting glucose levels in the model's decision-making process.\n \n   * HDL mg/dL: Holding an importance score of 0.047265, HDL cholesterol levels are the second most influential factor, indicating its relevance \n     in the   model's predictions.\n\n   * HOMA1_IR: The Homeostatic Model Assessment for Insulin Resistance (HOMA1_IR) has an importance score of 0.040456, highlighting its significance \n     in the model, especially in the context of metabolic health.\n\n   * TRIGLYCmg/dL: Triglyceride levels, with an importance score of 0.039053, also play a notable role in the model's predictions.\n\n   * Leptin (ng/mL): This hormone, which plays a key role in regulating energy balance, has an importance score of 0.038647, suggesting its importance \n     in the  predictive model.\n  \n  \n\n\n\n\n\n2. Least Influential Features:\n   Several features such as 'DG2', 'Diabetic Medication Taken', 'Meds for HTN Tapered', 'HYPERLIPIDEMIA PATIENT MEDICAL HISTORY', \n   and 'medication_old'  have an importance score of 0.000000. This indicates that these factors do not significantly influence the model's predictions in \n   the  current context.\n\n\n\n\n\nThis feature importance analysis provides crucial insights into the factors that the Random Forest model deems most relevant for making predictions. The prominence of features related to glucose levels, lipid profiles, and insulin resistance reflects the model's focus on metabolic health indicators. Understanding these key drivers can help in refining the model and focusing on the most impactful variables for predictive accuracy.\n\nIt is also important to note that the absence of influence from certain features does not necessarily mean they are irrelevant in general, but rather that they may not be significant within the specific context of this model and dataset.","metadata":{}},{"cell_type":"markdown","source":"**Partial Dependence Plots (PDPs):**\nThese plots show the relationship between a feature and the predicted outcome, holding all other features constant. They can help to understand the direction and magnitude of the influence of each feature.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.inspection import PartialDependenceDisplay\n\n# Correcting the approach to ensure feature names are correctly identified\n# Plotting Partial Dependence Plots for the top two features using the correct method\nfig, ax = plt.subplots(ncols=2, figsize=(15, 6))\nfeatures = [0, 1]  # Indices of the top two features in the scaled dataset\nPartialDependenceDisplay.from_estimator(rf_model, X_train_scaled, features, ax=ax, grid_resolution=20, feature_names=features)\nplt.suptitle('Partial Dependence Plots for TRIGLYCmg/dL and LDL CALCmg/dL')\nplt.show()","metadata":{"source_hash":null,"execution_start":1704222288467,"execution_millis":570,"deepnote_to_be_reexecuted":false,"cell_id":"0b7e87926837486bb110dd1439c4dd34","deepnote_cell_type":"code","execution":{"iopub.status.busy":"2024-02-15T14:37:39.355475Z","iopub.execute_input":"2024-02-15T14:37:39.355910Z","iopub.status.idle":"2024-02-15T14:37:39.968762Z","shell.execute_reply.started":"2024-02-15T14:37:39.355879Z","shell.execute_reply":"2024-02-15T14:37:39.967707Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Partial Dependence Plots for Key Features in Random Forest Model**\n\nObjective:\n\nIn this section, we aim to visualize the influence of the top two features on the predictions made by the Random Forest model. This is achieved through Partial Dependence Plots (PDP), which are powerful tools for interpreting complex models like Random Forest.\n\nImplementation Details:\n\n 1. Model: The Random Forest model ('rf_model') previously trained.\n \n 2. Data: The scaled training data ('X_train_scaled'), ensuring that the model's input is consistent with how it was trained.\n \n 3. Selected Features: We focus on the top two features, identified by their indices '[0, 1]' in the scaled dataset. These features are assumed to \n    be 'TRIGLYCmg/dL' and 'LDL CALCmg/dL' based on the title provided.\n    \n 4. Plotting Setup:\n     * A subplot grid is created with two columns ('ncols=2') to accommodate each feature's plot.\n     * The figure size is set to 15x6 inches for clear visualization.\n     \n \n 5. Partial Dependence Display:\n     * 'PartialDependenceDisplay.from_estimator' is the function used for plotting. It requires the model, dataset, features of interest, and \n        the axes object for plotting.\n     *  grid_resolution=20 indicates that the function will use 20 evenly spaced points along the range of each feature to calculate the \n        partial dependence.\n  \n  \n  \n 6. Feature Names: The feature names are passed as they are crucial for correctly labeling the axes in the plots.\n \n\n\nThe resulting plots will illustrate how changes in the values of 'TRIGLYCmg/dL' and 'LDL CALCmg/dL' independently affect the model's predictions, after accounting for the average effect of all other features in the model. This insight is valuable for understanding the behavior of the model and the relative importance of these features in its decision-making process.\n\n\nBy examining these plots, we can deduce trends and patterns such as:\n     - Does the prediction probability increase or decrease with increasing values of these features?\n     - Are there any thresholds or nonlinear relationships evident in these features?\n     \n     \nThis analysis is crucial for interpreting the model's inner workings and can guide future feature selection and engineering efforts.","metadata":{}},{"cell_type":"markdown","source":"**Summary and Conclusions**\n\n- Significance of Specific Health Metrics: The analysis provides insights into how particular health metrics, such as 'TRIGLYCmg/dL' and 'LDL CALCmg/dL', influence the predictions of a Random Forest model. This indicates that these metrics are significant in the context of the healthcare data being analyzed.\n\n- Understanding Model Behavior: The use of Partial Dependence Plots (PDPs) suggests that the analysis is focused on understanding the model's behavior about individual features. This indicates that the model may show certain trends or patterns in its predictions based on the values of these features.\n\n- Identification of Trends or Thresholds: The expected outcome of examining PDPs and other analyses is to identify any significant trends, thresholds, or non-linear relationships in how these health metrics affect model predictions. This could lead to the discovery of critical points where changes in these metrics significantly alter the predicted outcomes.\n\n- Guidance for Future Data Analysis and Feature Engineering: The conclusions drawn from this analysis can guide future feature selection and engineering efforts in similar healthcare datasets. Understanding which features are most influential in the model’s predictions can inform the development of more accurate and efficient predictive models.\n\n- (Possible)Implications for Healthcare Decisions: The insights gained from this analysis are not just of academic interest but might have practical implications for healthcare decision-making. By understanding the relationship between key health metrics and predicted outcomes, medical professionals can make more informed decisions regarding patient care and management.\n\n- Potential for Improved Patient Care: There are possible applications of this analysis in a real-world healthcare setting that could lead to improved patient care strategies. By identifying the most critical factors influencing health outcomes, healthcare providers can tailor their approaches to individual patient needs more effectively.","metadata":{}},{"cell_type":"markdown","source":" **References**\n  \n- [Novak, V., Quispe, R., & Saunders, C. (2022). Cerebral perfusion and cognitive decline in type 2 diabetes (version 1.0.1). PhysioNet. ](https://physionet.org/content/cerebral-perfusion-diabetes/1.0.1/)\n\n- [Novak V1, Zhao P, Manor B, Sejdic E, Alsop D, Abduljalil A, Roberson PK, Munshi M, Novak PDiabetes Care. 2011 Nov;34(11):2438-41. ](https://pubmed.ncbi.nlm.nih.gov/21926285/)\n\n- [Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215–e220. ](https://www.ahajournals.org/doi/full/10.1161/01.cir.101.23.e215)","metadata":{}}]}